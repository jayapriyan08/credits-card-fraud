# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MBOM7d0-8xc4gDSMV1UL_UngmrHCBxxl
"""

# Step 1: Upload the Dataset
from google.colab import files
uploaded = files.upload()

# Step 2: Load the Dataset
import pandas as pd

# Assuming the uploaded file is named 'student-mat.csv'
filename = next(iter(uploaded))  # Automatically gets the uploaded file's name
df = pd.read_csv(filename, sep=';')  # Adjust 'sep' if needed (e.g., ',' for most CSVs)

# Step 3: Data Exploration
print("First 5 rows of the dataset:")
print(df.head())

print("\nShape of the dataset:", df.shape)

print("\nColumn names:")
print(df.columns.tolist())

print("\nData types and non-null values:")
print(df.info())

print("\nSummary statistics for numeric features:")
print(df.describe())

# Step 4: Check for Missing Values and Duplicates
print("\nMissing values in each column:")
print(df.isnull().sum())

print("\nNumber of duplicate rows:", df.duplicated().sum())

"""# Task
for the attached file i want program

Here is all the data you need:
"credit_card_fraud_synthetic_hamad_phase2_updated_attached 111111.csv"

## Data loading

### Subtask:
Load the credit card fraud dataset.

**Reasoning**:
Load the dataset into a pandas DataFrame and display its basic information.
"""

import pandas as pd

try:
    df = pd.read_csv('credit_card_fraud_synthetic_hamad_phase2_updated_attached 111111.csv')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Error: 'credit_card_fraud_synthetic_hamad_phase2_updated_attached 111111.csv' not found.")
except pd.errors.ParserError:
    print("Error: Could not parse the CSV file. Please check the file format.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

"""## Data exploration

### Subtask:
Explore the loaded dataset to understand its characteristics.

**Reasoning**:
Examine data types, missing values, distributions, and relationships within the dataset to understand its characteristics, as per the instructions.
"""

# Data type examination
print("Data Types:\n", df.dtypes)

# Missing values
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100
print("\nMissing Values:\n", missing_values)
print("\nMissing Value Percentage:\n", missing_percentage)

# Numerical features distribution
numerical_features = df.select_dtypes(include=['number'])
print("\nNumerical Features Summary Statistics:\n", numerical_features.describe())
print("\nTarget Variable Distribution:\n", df['Is_Fraud'].value_counts(normalize=True))

# Relationship with target variable
print("\nCorrelation with Target Variable:\n", numerical_features.corrwith(df['Is_Fraud']))
# Categorical features vs. target variable
categorical_features = df.select_dtypes(include=['object'])

for col in categorical_features.columns:
    print(f'\nCross-tabulation for {col}:')
    display(pd.crosstab(df[col], df['Is_Fraud'], normalize='index'))


# Outliers
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_features.columns):
  plt.subplot(2, 2, i + 1)
  plt.boxplot(df[col])
  plt.title(col)
plt.tight_layout()
plt.show()

# Data shape
print("\nData Shape:", df.shape)

"""## Data cleaning

### Subtask:
Clean the data by handling potential outliers and duplicates.

**Reasoning**:
Clean the data by handling outliers in the 'Amount' column using the IQR method, remove duplicate rows, and verify the cleaning steps.
"""

import matplotlib.pyplot as plt

# Calculate IQR for 'Amount'
Q1 = df['Amount'].quantile(0.25)
Q3 = df['Amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Cap outliers
df['Amount'] = df['Amount'].clip(lower=lower_bound, upper=upper_bound)

# Remove duplicates
num_duplicates = df.duplicated().sum()
df.drop_duplicates(inplace=True)
print(f"Number of duplicate rows removed: {num_duplicates}")

# Verify cleaning
print("\nNumerical Features Summary Statistics after outlier handling:\n", df.describe())
print("\nTarget Variable Distribution after outlier handling:\n", df['Is_Fraud'].value_counts(normalize=True))

# Updated box plots
plt.figure(figsize=(10, 6))
plt.boxplot(df['Amount'])
plt.title('Amount after Outlier Handling')
plt.show()

print("\nMissing Values after cleaning:\n", df.isnull().sum())
print("\nShape of cleaned dataset:\n", df.shape)

"""## Data preparation

### Subtask:
Prepare the data for modeling by encoding categorical features and scaling numerical features.

**Reasoning**:
Identify categorical and numerical features, then encode categorical features using one-hot encoding and scale numerical features using standardization. Finally, concatenate all features into a single DataFrame.
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Identify categorical and numerical features
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numerical_cols = df.select_dtypes(include=['number']).columns.tolist()
numerical_cols.remove('Is_Fraud')

# One-hot encode categorical features
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='if_binary')
encoded_features = encoder.fit_transform(df[categorical_cols])
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols))

# Scale numerical features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[numerical_cols])
scaled_df = pd.DataFrame(scaled_features, columns=numerical_cols)

# Concatenate encoded categorical features, scaled numerical features and target variable
df_prepared = pd.concat([encoded_df, scaled_df, df['Is_Fraud']], axis=1)
display(df_prepared.head())

"""## Data splitting

### Subtask:
Split the prepared data into training, validation, and testing sets.

**Reasoning**:
Split the prepared data into training, validation, and testing sets using train_test_split, while maintaining class distribution and setting a random state for reproducibility.
"""

from sklearn.model_selection import train_test_split

# Split data into training and combined validation/testing sets
X = df_prepared.drop('Is_Fraud', axis=1)
y = df_prepared['Is_Fraud']
X_train, X_val_test, y_train, y_val_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Further split combined validation/testing sets into validation and testing sets
X_val, X_test, y_val, y_test = train_test_split(
    X_val_test, y_val_test, test_size=0.5, random_state=42, stratify=y_val_test
)

# Verify the shapes and class distributions
print("Training set shape:", X_train.shape, y_train.shape)
print("Validation set shape:", X_val.shape, y_val.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

print("\nTraining set target distribution:\n", y_train.value_counts(normalize=True))
print("\nValidation set target distribution:\n", y_val.value_counts(normalize=True))
print("\nTesting set target distribution:\n", y_test.value_counts(normalize=True))

"""## Model training

### Subtask:
Train classification models on the training data.

**Reasoning**:
Train Logistic Regression, Random Forest, and Gradient Boosting classifiers on the training data.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# Initialize classifiers
log_reg = LogisticRegression(max_iter=1000)  # Increased max_iter to ensure convergence
random_forest = RandomForestClassifier(random_state=42)
gradient_boosting = GradientBoostingClassifier(random_state=42)

# Train the models
trained_models = {}
trained_models['Logistic Regression'] = log_reg.fit(X_train, y_train)
trained_models['Random Forest'] = random_forest.fit(X_train, y_train)
trained_models['Gradient Boosting'] = gradient_boosting.fit(X_train, y_train)

"""## Model optimization

### Subtask:
Optimize the hyperparameters of the trained classification models using the validation set.

**Reasoning**:
Optimize the hyperparameters of the trained classification models using the validation set.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, make_scorer

# Define parameter grids for each model
param_grid_logreg = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
param_grid_gb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10]
}

# Use f1_score as the evaluation metric due to class imbalance
f1_scorer = make_scorer(f1_score, average='weighted')

# Perform GridSearchCV for each model
best_models = {}
for model_name, model in trained_models.items():
    if model_name == 'Logistic Regression':
        grid_search = GridSearchCV(model, param_grid_logreg, scoring=f1_scorer, cv=5)
    elif model_name == 'Random Forest':
        grid_search = GridSearchCV(model, param_grid_rf, scoring=f1_scorer, cv=5)
    elif model_name == 'Gradient Boosting':
        grid_search = GridSearchCV(model, param_grid_gb, scoring=f1_scorer, cv=5)
    else:
        continue  # Skip if the model is not in the list

    grid_search.fit(X_val, y_val)
    best_models[model_name] = {
        'model': grid_search.best_estimator_,
        'params': grid_search.best_params_,
        'score': grid_search.best_score_
    }
    print(f"Best parameters for {model_name}: {grid_search.best_params_}")
    print(f"Best cross-validation score for {model_name}: {grid_search.best_score_}")

"""## Model evaluation

### Subtask:
Evaluate the performance of the best models on the test set.

**Reasoning**:
Evaluate the performance of the best models on the test set using the provided metrics.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import pandas as pd

# Assuming 'best_models' is defined from the previous step
evaluation_results = []

for model_name, model_data in best_models.items():
    model = model_data['model']
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    roc_auc = roc_auc_score(y_test, y_prob)

    evaluation_results.append([model_name, accuracy, precision, recall, f1, roc_auc])

evaluation_df = pd.DataFrame(evaluation_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC-ROC'])
display(evaluation_df)

# Analyze the results
print("\nAnalysis of Model Performance:")
for index, row in evaluation_df.iterrows():
  print(f"\nModel: {row['Model']}")
  print(f"Accuracy: {row['Accuracy']:.4f}")
  print(f"Precision: {row['Precision']:.4f}")
  print(f"Recall: {row['Recall']:.4f}")
  print(f"F1-score: {row['F1-score']:.4f}")
  print(f"AUC-ROC: {row['AUC-ROC']:.4f}")

"""**Reasoning**:
Fix the error in the previous code block by specifying the `multi_class` parameter in `roc_auc_score` and handling potential `UndefinedMetricWarning`.  Also, adjust the code to correctly handle multi-class classification for other metrics.


"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import pandas as pd
import warnings

# Assuming 'best_models' is defined from the previous step
evaluation_results = []

for model_name, model_data in best_models.items():
    model = model_data['model']
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)

    accuracy = accuracy_score(y_test, y_pred)

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    if y_prob.shape[1] > 2:  # Handle multi-class case for roc_auc
      roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovo')
    else:
      roc_auc = roc_auc_score(y_test, y_prob[:, 1])


    evaluation_results.append([model_name, accuracy, precision, recall, f1, roc_auc])

evaluation_df = pd.DataFrame(evaluation_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC-ROC'])
display(evaluation_df)

# Analyze the results
print("\nAnalysis of Model Performance:")
for index, row in evaluation_df.iterrows():
  print(f"\nModel: {row['Model']}")
  print(f"Accuracy: {row['Accuracy']:.4f}")
  print(f"Precision: {row['Precision']:.4f}")
  print(f"Recall: {row['Recall']:.4f}")
  print(f"F1-score: {row['F1-score']:.4f}")
  print(f"AUC-ROC: {row['AUC-ROC']:.4f}")

"""## Summary:

### Q&A

No questions were explicitly asked in the provided text. However, the analysis implicitly addresses questions such as:

* **What is the nature of the data?** The data contains information about credit card transactions, including details like transaction amount, location, merchant type, and a fraud indicator.
* **Are there missing values or outliers?** Initial exploration revealed no missing values. Outliers were present in the 'Amount' column and were addressed by capping them. Duplicated rows were removed.
* **How do features relate to the target variable (fraud)?**  Correlations and cross-tabulations explored the relationships between numerical and categorical features and the target variable.
* **Which model performs best for fraud detection?** Model evaluation using metrics like accuracy, precision, recall, F1-score, and AUC-ROC compared the performance of Logistic Regression, Random Forest, and Gradient Boosting. Random Forest showed the best AUC-ROC score.


### Data Analysis Key Findings

* **No Missing Data:**  The dataset initially contained no missing values.
* **Outlier Handling:** Outliers in the 'Amount' column were capped using the IQR method.  Zero duplicate rows were found and removed.
* **Class Imbalance:** The target variable ('Is_Fraud') exhibited a class imbalance, with a majority of non-fraudulent transactions.  This was addressed by stratification during data splitting and using weighted average for evaluation metrics.
* **Feature Engineering:** Categorical features were one-hot encoded, and numerical features were standardized.
* **Model Performance:** All three models (Logistic Regression, Random Forest, Gradient Boosting) achieved similar accuracy, precision, recall, and F1-score on the test set (~0.90, ~0.82, ~0.91 and ~0.86 respectively).  However, Random Forest achieved the highest AUC-ROC score (0.6435) on the test set, indicating potentially better performance in distinguishing between classes.  The other two models had lower AUC-ROC scores.
* **Hyperparameter Tuning Results:**  All models achieved the same best cross-validation score (0.8433) on the validation set, which raises concerns about the robustness of the results or possible overfitting. The limited number of samples in some classes triggered a UserWarning related to `n_splits` in GridSearchCV.

### Insights or Next Steps

* **Investigate Hyperparameter Tuning:** The identical best cross-validation scores across all models suggest that the current hyperparameter grids might be insufficient or that the data itself might be limiting model discrimination.  Investigate a wider range of hyperparameters, explore different optimization techniques, or consider more sophisticated evaluation metrics.
* **Address Class Imbalance Further:** Although stratification and weighted averages were used, consider more advanced techniques like SMOTE or cost-sensitive learning to address class imbalance more effectively, especially as the class sizes are small.  Re-evaluate the performance on the test set with the new techniques.

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import numpy as np

# Load the dataset
df = pd.read_csv("credit_card_fraud_synthetic_hamad_phase2_updated_attached 111111.csv")

# Convert Time to hours
df['Hour'] = (df['Time'] // 3600) % 24
sns.set(style="whitegrid")

# ───────────── MODULE 1: Fraud Frequency ─────────────
plt.figure(figsize=(12, 5))
sns.countplot(data=df, x='Hour', hue='Is_Fraud', palette='Set2')
plt.title("Fraud Frequency Across Hours of Day")
plt.xlabel("Hour of Day")
plt.ylabel("Transaction Count")
plt.legend(title="Is Fraud")
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=df, x='Amount', hue='Is_Fraud', bins=50, kde=True, palette='Set1', element='step')
plt.title("Transaction Amount Distribution by Fraud Status")
plt.xlabel("Transaction Amount")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# ───────────── MODULE 2: Correlation ─────────────
numeric_features = df.select_dtypes(include=np.number)
corr = numeric_features.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Correlation Matrix of Numeric Features")
plt.tight_layout()
plt.show()

# ───────────── MODULE 3: Outlier & Clustering ─────────────
features = ['Amount', 'Time']
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[features])

kmeans = KMeans(n_clusters=3, random_state=42)
df['Cluster'] = kmeans.fit_predict(df_scaled)

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Amount', y='Time', hue='Cluster', data=df, palette='Set1')
plt.title("Transaction Clustering Based on Amount and Time")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
sns.boxplot(data=df, x='Is_Fraud', y='Amount')
plt.title("Outliers in Transaction Amounts by Fraud Status")
plt.tight_layout()
plt.show()